{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from fuzzywuzzy import fuzz\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import random \n",
    "\n",
    "from knowledge_graph import KG\n",
    "from task_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../data/food/construction...\n",
      "Loaded 1001498 triples.\n"
     ]
    }
   ],
   "source": [
    "kg = KG(\"data/food/construction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: entity_matching\n",
      "entity_matching (train): 12920 samples\n",
      "dict_keys(['train'])\n",
      "entity_matching (valid): 1615 samples\n",
      "dict_keys(['train', 'valid'])\n",
      "entity_matching (test): 2465 samples\n",
      "dict_keys(['train', 'valid', 'test'])\n",
      "\n",
      "Task: arc_matching\n",
      "arc_matching (train): 0 samples\n",
      "dict_keys(['train'])\n",
      "arc_matching (valid): 0 samples\n",
      "dict_keys(['train', 'valid'])\n",
      "arc_matching (test): 50 samples\n",
      "dict_keys(['train', 'valid', 'test'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kg.load_tasks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Matching Task\n",
    "\n",
    "Our baseline consists of string matching on names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates(triples, test_x):\n",
    "    \"\"\" The entity matching task consists of mapping USDA entities to Open Food Facts (OFF) entities. \n",
    "    In order to make comparisons feasible, we first recommend performing a candidate generation step, where\n",
    "    we generate a list of likely USDA candidates for every OFF entity we seek to match.\n",
    "    \n",
    "    Args:\n",
    "        triples (list): list of all triples in the KG\n",
    "        test_x: all unknown USDA entities in the test split\n",
    "    \"\"\"\n",
    "    off2usda_arcs = {\n",
    "        'name': ['name', 'long_name'],\n",
    "        'product_name': ['name', 'long_name'],\n",
    "        'type': ['type']\n",
    "    }\n",
    "    \n",
    "    arcs2save = set()\n",
    "    for k, v in off2usda_arcs.items():\n",
    "        arcs2save.add(k)\n",
    "        for vv in v:\n",
    "            arcs2save.add(vv)\n",
    "        \n",
    "    entity2source = {} \n",
    "    entity2features = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    # collect features for each entity \n",
    "    print(\"Collecting Features\")\n",
    "    for head, arc, tail, tail_type, source in tqdm(triples):\n",
    "        entity2source[head] = source\n",
    "        if arc in arcs2save:\n",
    "            entity2features[head][arc].append(tail)\n",
    "    \n",
    "    # generate candidates \n",
    "    candidate_scores = defaultdict(lambda: defaultdict(int))\n",
    "    print(\"Scoring candidates\")\n",
    "    \n",
    "    for off_entity in tqdm(test_x): \n",
    "        for candidate_entity in entity2features:\n",
    "            if not candidate_entity in entity2features:\n",
    "                continue\n",
    "                \n",
    "            if not off_entity in entity2features:\n",
    "                continue\n",
    "\n",
    "            # skip candidate if it belongs to USDA\n",
    "            if entity2source[candidate_entity] == 'OFF':\n",
    "                continue \n",
    "            \n",
    "            candidate_features = entity2features[candidate_entity]\n",
    "            # skip entity if it's not a food product\n",
    "            if not 'food_product' in candidate_features['type']:\n",
    "                continue\n",
    "            \n",
    "    \n",
    "            '''print(\"Source entity:\",off_entity, entity2source[off_entity])\n",
    "            print(\"Source features:\",entity2features[off_entity])\n",
    "            print(\"Candidate:\", candidate_entity, entity2source[candidate_entity])\n",
    "            print(\"Candidate features:\", candidate_features)'''\n",
    "            score = 0\n",
    "            for off_feature_name, usda_feature_name_list in off2usda_arcs.items():\n",
    "                max_score = 0\n",
    "                for usda_feature_name in usda_feature_name_list:\n",
    "                    for candidate_val in candidate_features.get(usda_feature_name, []):\n",
    "                        for off_val in entity2features[off_entity].get(off_feature_name, []):\n",
    "                            #print(off_feature_name, off_val, usda_feature_name, candidate_val, fuzz.ratio(off_val, candidate_val))\n",
    "                            max_score = max(fuzz.ratio(off_val, candidate_val), max_score)\n",
    "                score += max_score \n",
    "            candidate_scores[off_entity][candidate_entity] = score\n",
    "    return candidate_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 224072/1001498 [00:00<00:00, 1108640.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1001498/1001498 [00:00<00:00, 1192017.74it/s]\n",
      "  0%|          | 1/2465 [00:00<05:58,  6.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring candidates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2465/2465 [05:24<00:00,  6.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get candidates\n",
    "candidates = generate_candidates(kg.triples, kg.tasks['entity_matching']['test']['X'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict top scoring candidates \n",
    "unfiltered_predictions = {} \n",
    "all_scores = []\n",
    "for off_entity in kg.tasks['entity_matching']['test']['X']:\n",
    "    max_score = 0\n",
    "    max_candidate = \"\"\n",
    "    for candidate, score in candidates[off_entity].items():\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            max_candidate = candidate\n",
    "    all_scores.append(max_score)\n",
    "    unfiltered_predictions[off_entity] = (max_candidate, max_score)\n",
    "\n",
    "# use median score as threshold \n",
    "threshold = sorted(all_scores)[int(len(all_scores) / 2)]\n",
    "\n",
    "filtered_predictions = {}\n",
    "for off_entity, candidate_tuple in unfiltered_predictions.items():\n",
    "    candidate, score = candidate_tuple\n",
    "    if score < threshold:\n",
    "        filtered_predictions[off_entity] = \"None\"\n",
    "    else:\n",
    "        filtered_predictions[off_entity] = candidate\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.046661. Recall: 0.068155. F1: 0.055396\n"
     ]
    }
   ],
   "source": [
    "X = kg.tasks['entity_matching']['test']['X']\n",
    "Y = kg.tasks['entity_matching']['test']['Y']\n",
    "precision, recall, f1 = evaluate_entity_matching(filtered_predictions, X, Y)\n",
    "print(\"Precision: %f. Recall: %f. F1: %f\" % (precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arc Matching \n",
    "\n",
    "We will align arcs for known paired entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_entities = kg.tasks['entity_matching']['train']['X']\n",
    "usda_entities = kg.tasks['entity_matching']['train']['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7b43042a644741981ce6a5d2252d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "off_arcs = kg.tasks['arc_matching']['test']['X']\n",
    "arc_scores = defaultdict(lambda: defaultdict(int))\n",
    "pairs = list(zip(off_entities, usda_entities))\n",
    "random.shuffle(pairs)\n",
    "pairs = pairs[:1000]\n",
    "for off_e, usda_e in tqdm_notebook(pairs):\n",
    "    off_triples = kg.filter_triples(head_filter=[off_e])\n",
    "    usda_triples = kg.filter_triples(head_filter=[usda_e])\n",
    "    for _, usda_arc, usda_tail, usda_tail_type, _ in usda_triples: \n",
    "        if usda_tail_type == 'entity':\n",
    "            if not usda_tail in entity2name:\n",
    "                continue\n",
    "            usda_tail = entity2name[usda_tail]\n",
    "        for _, off_arc, off_tail, off_tail_type, _ in off_triples: \n",
    "            if not off_arc in off_arcs:\n",
    "                continue \n",
    "            if off_tail_type == 'entity':\n",
    "                if not off_tail in entity2name:\n",
    "                    continue\n",
    "                off_tail = entity2name[off_tail]\n",
    "            arc_scores[off_arc][usda_arc] = fuzz.ratio(usda_tail, off_tail)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for arc in off_arcs:\n",
    "    best_score = 0\n",
    "    best_candidate = \"\"\n",
    "    for usda_arc, score in arc_scores[arc].items():\n",
    "        if score > best_score:\n",
    "            best_score = score \n",
    "            best_candidate = usda_arc\n",
    "    if best_score > 90:\n",
    "        predictions[arc] = best_candidate\n",
    "    else:\n",
    "        predictions[arc] = \"None\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.238095. Recall: 0.147059. F1: 0.181818\n"
     ]
    }
   ],
   "source": [
    "X = kg.tasks['arc_matching']['test']['X']\n",
    "Y = kg.tasks['arc_matching']['test']['Y']\n",
    "precision, recall, f1 = evaluate_arc_matching(predictions, X, Y)\n",
    "print(\"Precision: %f. Recall: %f. F1: %f\" % (precision, recall, f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
